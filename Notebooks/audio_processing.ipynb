{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import glob, pickle, yaml\n",
    "\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from dataloader import make_supervised_dataset\n",
    "from callbacks import CustomWandbCallback, ModelCheckpoint\n",
    "from train_supervised import make_supervised_model\n",
    "from timbre_transfer import transfer_timbre_from_path, load_model_from_config\n",
    "\n",
    "def print_plot_play(x, Fs=16000, text='', normalize=False):\n",
    "    import IPython.display as ipd\n",
    "    print('%s\\n' % (text))\n",
    "    print('Fs = %d, x.shape = %s, x.dtype = %s' % (Fs, x.shape, x.dtype))\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot(x, color='gray')\n",
    "    plt.xlim([0, x.shape[0]])\n",
    "    plt.xlabel('Time (samples)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    ipd.display(ipd.Audio(data=x, rate=Fs, normalize=normalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../wandb/run-20210818_235757-tknchpfs/files/Supervised_Violin/Supervised_Violin.yaml'\n",
    "config_path = \"../wandb/run-20210818_235746-36ggudop/files/Supervised_Latent_Violin/Supervised_Latent_Violin.yaml\"\n",
    "#config_path = \"/kuacc/users/hbalim15/ddsp/wandb/run-20210817_170423-5v6s3iu1/files/Supervised_Violin_/Supervised_Violin_.yaml\"\n",
    "with open(config_path) as file:\n",
    "    config = dict(yaml.load(file, Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model_from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Timbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../audio_clips/singing.mp3\"\n",
    "resynth = transfer_timbre_from_path(model, input_path, pitch_shift=24, mfcc=config['model']['encoder'])\n",
    "print_plot_play(resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_title = \"violin_to_violin-VIDOuble.wav\"\n",
    "write_audio(resynth, output_title, RUN_NAME, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",RUN_NAME = 'Supervised_Violin_ouz_Multiloss'\n",
    "\n",
    "encoder_timesteps = 250\n",
    "decoder_timesteps = 1000\n",
    "\n",
    "preprocessor = F0LoudnessPreprocessor(timesteps=encoder_timesteps)\n",
    "encoder = None #SupervisedEncoder(timesteps=encoder_timesteps)\n",
    "decoder = DecoderWithoutLatent(timesteps=decoder_timesteps)\n",
    "loss = MultiLoss() # SpecLoss()\n",
    "tracker_names = ['spec_loss'] if loss.name=='SpecLoss' else ['spec_loss', 'perc_loss', 'total_loss']\n",
    "model = SupervisedAutoencoder(preprocessor=preprocessor,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            loss_fn=loss,\n",
    "                            tracker_names=tracker_names,\n",
    "                            add_reverb=True)\n",
    "model.load(RUN_NAME+\"/994/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessing import process_track\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_title = \"singing.mp3\" \n",
    "track, _ = librosa.load(os.path.join(\"../audio_clips\", '{}'.format(input_title)),sr=16000)\n",
    "f = process_track(track,mfcc=config['model']['encoder'], pitch_shift=2, model=model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['loudness_db'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = process_track(track, mfcc=True, loudness_nfft=2048, frame_size=64000, Fs=16000, frame_rate=250, pitch_shift=2, model=None, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something about the loudness ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['inputs']['f0_scaled'].numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['inputs']['f0_hz'].numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot_play(resynth/resynth.max(),16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_plot_play(resynth/resynth.max(),16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track, fs = librosa.load(\"audio_clips/singing.mp3\",sr=16000)\n",
    "track_T = librosa.effects.pitch_shift(track, 16000, n_steps=24)\n",
    "\n",
    "features = feature_extractor(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(features[\"loudness_db\"].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import make_datasets, make_violin_set\n",
    "train,_,_ = make_violin_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness = []\n",
    "for batch in iter(train):\n",
    "    loudness.append(batch[\"loudness_db\"])\n",
    "loudness = np.array(loudness).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(loudness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_spectrogram\n",
    "plot_waveform_spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dB_spectrogram = extract_dB_spectrogram(resynth, 8192, 1024, 512, center=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8), nrows=2, sharex=True, constrained_layout=True) #, dpi=50\n",
    "\n",
    "librosa.display.specshow(dB_spectrogram, sr=fs, hop_length=512, x_axis='time', y_axis='log', ax=ax[0])\n",
    "\n",
    "librosa.display.waveplot(resynth, sr=fs, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length=512\n",
    "dB_spectrogram = extract_dB_spectrogram(track, 8192, 1024, hop_length, center=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "librosa.display.specshow(dB_spectrogram, sr=16000, hop_length=hop_length, x_axis='time', y_axis='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot_play(track_T/track_T.max(),fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot_play(resynth/resynth.max(),fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot_play(track/track.max(), fs,'track')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the reconstructed 4*1sec frames are already frames inside we just concat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windowed_frames(x, frame_size, window_type):\n",
    "\n",
    "    audio_frames = frame_generator(x, frame_size=frame_size)\n",
    "\n",
    "    window = signal.get_window(window_type, frame_size, fftbins=False)\n",
    "\n",
    "    windowed_frames = [frame*window for frame in audio_frames]\n",
    "    \n",
    "    return windowed_frames\n",
    "\n",
    "def reconstruct(windowed_frames, frame_size):\n",
    "    \"\"\"\n",
    "    Overlap-add method with 50% overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    reconstruction = [windowed_frames[0][:frame_size//2]] # first frame's beginning\n",
    "    for i in range(len(windowed_frames)-1):\n",
    "\n",
    "         reconstruction += [windowed_frames[i][frame_size//2:] + windowed_frames[i+1][:frame_size//2]]\n",
    "\n",
    "    reconstruction += [windowed_frames[i][frame_size//2:]] # last frames end\n",
    "\n",
    "    reconstruction = np.array(reconstruction).reshape(-1)\n",
    "    \n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each audio_synth frame is overlapp-added separately and concat at the end\n",
    "fs = 16000\n",
    "frame_size = int((8/1000)*fs)\n",
    "\n",
    "separate_reconstruction = []\n",
    "for synth in audio_synth:\n",
    "    \n",
    "    windowed_frames = generate_windowed_frames(synth, frame_size, 'hamming')\n",
    "    \n",
    "    reconstruction = reconstruct(windowed_frames, frame_size)\n",
    "    \n",
    "    separate_reconstruction.append(reconstruction)\n",
    "    \n",
    "separate_reconstruction = np.array(separate_reconstruction).reshape(-1)\n",
    "\n",
    "separate_reconstruction /= max(separate_reconstruction) # normalize\n",
    "\n",
    "\n",
    "# all audio_synth frames are merged and overlap-added together\n",
    "\n",
    "windowed_frames = generate_windowed_frames(audio_synth.reshape(-1), frame_size, 'hamming')\n",
    "\n",
    "joint_reconstruction = reconstruct(windowed_frames, frame_size)\n",
    "\n",
    "joint_reconstruction /= max(joint_reconstruction)\n",
    "\n",
    "\n",
    "# simple concat\n",
    "\n",
    "simple_reconstruction = audio_synth.reshape(-1)\n",
    "\n",
    "simple_reconstruction /= max(simple_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "k_size = 5\n",
    "#smooth = scipy.signal.convolve(audio_synth.reshape(-1,),np.ones(k_size)/k_size)\n",
    "smooth = audio_synth.reshape(-1,)\n",
    "norm = smooth/smooth.max()\n",
    "print_plot_play(norm, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "write(\"reverb.wav\", 16000, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_plot_play(joint_reconstruction, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot_play(separate_reconstruction, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*250"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
